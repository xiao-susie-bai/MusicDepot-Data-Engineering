{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part 2 - Data Quality and Orchestration\n",
    "\n",
    "For the second part of this project, we will further develop the data pipeline. I will integrate data quality checks and orchestration to improve the robustness of the architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [ 1 - Introduction](#1)\n",
    "- [ 2 - Deployment of the Previous Architecture](#2)\n",
    "- [ 3 - Data Quality with AWS Glue](#3)\n",
    "  - [ 3.1 - Configuring the Rule Sets](#3-1)\n",
    "  - [ 3.2 - Creating Materialized Views with *dbt*](#3-2)\n",
    "- [ 4 - Orchestration with Apache Airflow](#4)\n",
    "  - [ 4.1 - Accessing Apache Airflow](#4-1)\n",
    "  - [ 4.2 - DAG for Songs Data in RDS Source](#4-2)\n",
    "  - [ 4.3 - DAG for Users and Sessions Data from API Source](#4-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Introduction\n",
    "\n",
    "Music Depot is a new company in the music industry, offering a subscription-based app for streaming songs. Recently, they have expanded their services to include digital song purchases. With this new retail feature, Music Depot requires a data pipeline to extract purchase data from their new API and operational database, enrich and model this data, and ultimately deliver the comprehensive data model for the Marketing Analytics team to review and gain insights. In the first part, I have developed an initial version of the pipeline. Now here are some new requirements to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new requirements for this project are:\n",
    "\n",
    "1. The pipeline should allow for incremental ingestion of new data from the data sources.\n",
    "2. The pipeline should run daily using data orchestration (we will use Airflow).\n",
    "3. Data quality checks should be implemented to verify the quality of newly ingested and cleansed data.\n",
    "4. Analytical views should be added on top of the star schema data model.\n",
    "5. A company dashboard would be added to the architecture to visualize analytical views and insights (by the Marketing Analytics team)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Deployment of the Previous Architecture\n",
    "\n",
    "I will recreate the data architecture from the first part of the project; this is a refresher of the elements:\n",
    "\n",
    "\n",
    "1. **Data Sources**:\n",
    "   1. *Music Depot API*: Contains the `users` and `sessions` endpoints, used to gather information about sales parametrized by a start and end date.\n",
    "   2. *Music Depot Operational RDS*: Contains the `songs` table, with all the information related to the available songs in the platform\n",
    "2. **Extract Jobs**: Three AWS Glue jobs in charge of extracting information from the data sources into the `landing zone`. A new argument has been added to the jobs known as `ingest_date`, this will help with the incremental load of new data.\n",
    "3. **Transform Jobs**: Two AWS Glue jobs in charge of transforming the raw information coming from the `landing zone` and saving the cleansed data into Apache Iceberg tables in the `transformation zone`.\n",
    "4. **Redshift Spectrum + Glue Catalog**: Enable Redshift to query Apache Iceberg tables in the `transformation zone`\n",
    "5. **Redshift**: Data warehouse solution used as the `serving layer`. The data modelling is done using *dbt*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy this infrastructure again, I have reused some Terraform files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. This project is built on AWS infrastructure. Run the following code to get the link to the AWS console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://signin.aws.amazon.com/federation?Action=login&SigninToken=PcboJ1a10Obw_YCNJY2d2ISxICN-sE4n_ae_XcLpRjVfqMw-K0AH_UyqElSkneuv0ARYj2Lyec_vtauhXJuHjY5zRMEcG7nLdLAJTgpGmO4keuRbeCpJbnBnDaG7hZMLdYxg7MnGuDim-aSTn715iUKh_lYN-yU4OSw6UfuJm26BUUKOxg4c6o-tNXnCpKQsPX0dsxaj2uLjx8-exFCVLJPCcRVOS9m54BVb-5H-8Ns3e7HGT8HD3a9njzKp-pwLnwVdUjXe1Rk6WO2WLoB5HizGR1ebTAHIPdlNsbGGFxsjvsaZSKfmivT42dAqOe8pjToqXc1hUc7s7MiRJXjPFJBemQvG4SsiJnei9Zrbq4lX67LrlPiGiu17HI8XtrzeZRMigPCGFhK_8ZYPZrTLL3D0mxJtYtRrmjKMq7kdVKW-z-D7V9_4YfLHaiC9-ByJL6VoLEmQrnl4HL3vFfb10rQ-DDPSM-Y-Enjzkh2DasKN_xibk65EsvzuLz87DtEIvRdgIrEYPXE7HWXM_-axqxTJJ_4UD6alQ-MTbv8xDjzKnnyk7e56iyf05PUbU2cTuvJzBxzawqHFbADPNhoBkWMvgIqC5oOiDn40wNNRSVpX8bTGB-8R209zvLqRME2OONbknx32q5kXhLA-lcnI-28i_naAcFnRpAP-xgg3aX8LAB67wd2Ds5VJK0ls6oLYdxYw4nyXN4Upfsxk5lvvDhetbQ3-wwuoxh16yIkr7VLgvw54s1khTx8pYqBULrAIPLHf85AWu6Qjjwt8_j6RKAjG1NjdIqA-9_5DeQBpezxqjxUXwbMeIOwkwj6I-IkEXi7Hg0IGxErubC4bYDh0G35i9Gmz8G_qwhIPcXzVpJF2-_w2--XmtoNHDT_t67ApBYkBd4UPWzoUe_yquQpfb_lrmcZvLZ5oU1Oumilt0dg9jRJsJKI9kitmJNQpc3VnFmENnPBfgEzveAOInryXkJwc-CPHuWU6nF-SLISF763XbvCNTiBupH74K5rqL9uZ4MR0l6Nutb8H7LarB8x7nnWdt9JPn08AGAb26F6dck_LF1NFQJ7QY9D2Pu5a8pUY2xNqXEPBt-YRk7PNQ4uftwjXqB8QidKSGt5ZX_8096mLj3ZYO4VbrSEZkVLeqsGo8F6TMeN9EbC9SZu92dFS6u4XnKq4y9_3cNqxG-S4ND274kawUBuUfutUDQmT26cuUxuCBAnjC54W_dBrEe0DeZ9jnqPktK_AT4W4zFGFBqF2QUFYMVK31DaJ8Moh2gKckpQpSm1LZXJ8MKywKrzDM-QbpPXCh9bv_L30xoEMM1jBjKZTc_AO0BZljRs-owfQ2rmzACphXIWwpEtcM6auN8Lq2ppP60llU7_Mtbnnhky1pvvXaOhcfaLASKVRYl6x5ivOeA2F-cBy8-z63p9v9ogDkZvEeR-u-GVQEz94A6tECtJocbIhIJY&Issuer=https%3A%2F%2Fapi.vocareum.com&Destination=https%3A%2F%2Fconsole.aws.amazon.com%2Fconsole%2Fhome%3Fregion%3Dus-east-1\" target=\"_blank\">GO TO AWS CONSOLE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "with open('../.aws/aws_console_url', 'r') as file:\n",
    "    aws_url = file.read().strip()\n",
    "\n",
    "HTML(f'<a href=\"{aws_url}\" target=\"_blank\">GO TO AWS CONSOLE</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the AWS console and navigate to **CloudFormation**. Click on the alphanumeric stack name and then go to the **Outputs** tab. We will see the key `APIEndpoint` - Use this **Value** in the glue file `terraform/modules/extract_job/glue.tf` correspondingly. \n",
    "\n",
    "In the `terraform/modules/extract_job/glue.tf` file, take a look at the ingestion date which is in the format `YYYY-MM-DD`. Jobs will be running each month to extract data from the previous month, starting by ingesting the data of January 2020. So the ingestion date will need to be the first day of the next month, which is supposed to be `\"2020-02-01\"` here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. In the glue file `terraform/modules/transform_job/glue.tf`, we have the same next ingestion date (`\"2020-02-02\"`) in the format `YYYY-MM-DD`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. In the terminal we run the following command to set up the environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "source scripts/setup.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, enter the `terraform` folder. We will apply the terraform plan module by module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "cd terraform\n",
    "terraform init\n",
    "terraform plan\n",
    "terraform apply -target=module.extract_job\n",
    "terraform apply -target=module.transform_job\n",
    "terraform apply -target=module.serving\n",
    "```\n",
    "*Note*: Remember to reply `yes` to the command `terraform apply` in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: If there are errors in the commands or Terraform configuration files, the terminal may crash. \n",
    "\n",
    "We can reopen the terminal by pressing <code>Ctrl + \\`</code> (or <code>Cmd + \\`</code>) or by navigating to View > Terminal. \n",
    "In the terminal, go again to the Terraform folder (`cd terraform`) and then try \n",
    "rerunning the required commands. The error should now appear in the terminal.\n",
    "If the terminal continues to crash, run the following command instead:\n",
    "`terraform apply -no-color  2> errors.txt`\n",
    "This will create a text file containing the error message without causing the terminal to crash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. Perform an initial run of the glue jobs contained in the Terraform outputs. First, we need to run three jobs, the names of which are in the following output variables: `glue_api_users_extract_job`, `glue_sessions_users_extract_job` and `glue_rds_extract_job`. Use the following command template in the terminal to execute each job based on its name; Or, manually click to start the AWS Glue jobs in the AWS Management Console.\n",
    "\n",
    "```bash\n",
    "aws glue start-job-run --job-name <JOB-NAME> | jq -r '.JobRunId'\n",
    "```\n",
    "\n",
    "You should get `JobRunID` in the output. Use this job run ID to track each job status by using this command:\n",
    "\n",
    "```bash\n",
    "aws glue get-job-run --job-name <JOB-NAME> --run-id <JobRunID> --output text --query \"JobRun.JobRunState\"\n",
    "```\n",
    "\n",
    "Wait until the three jobs statuses change to `SUCCEEDED` (each job should take around 3 mins). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5. Now run two jobs, the names of which are in the following outputs: `glue_json_transformation_job` and `glue_songs_transformation_job`. Follow the same steps as in the previous cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6. Let's verify that the data is available connecting to the Redshift cluster. Use the value of `RedshiftClusterEndpoint` key from the CloudFormation stack to get the connection; then run the code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDSHIFTDBHOST = 'de-c4w4a2-redshift-cluster.ccr4tux8asfe.us-east-1.redshift.amazonaws.com'\n",
    "REDSHIFTDBPORT = 5439\n",
    "REDSHIFTDBNAME = 'dev'\n",
    "REDSHIFTDBUSER = 'defaultuser'\n",
    "REDSHIFTDBPASSWORD = 'Defaultuserpwrd1234+'\n",
    "\n",
    "redshift_connection_url = f'postgresql+psycopg2://{REDSHIFTDBUSER}:{REDSHIFTDBPASSWORD}@{REDSHIFTDBHOST}:{REDSHIFTDBPORT}/{REDSHIFTDBNAME}'\n",
    "%sql {redshift_connection_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7. Verify the existing schemas, among them should be the `deftunes_transform` and `deftunes_serving`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql+psycopg2://defaultuser:***@de-c4w4a2-redshift-cluster.ccr4tux8asfe.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "4 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>database_name</th>\n",
       "        <th>schema_name</th>\n",
       "        <th>schema_owner</th>\n",
       "        <th>schema_type</th>\n",
       "        <th>schema_acl</th>\n",
       "        <th>source_database</th>\n",
       "        <th>schema_option</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>dev</td>\n",
       "        <td>deftunes_serving</td>\n",
       "        <td>100</td>\n",
       "        <td>local</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>dev</td>\n",
       "        <td>deftunes_transform</td>\n",
       "        <td>100</td>\n",
       "        <td>external</td>\n",
       "        <td>None</td>\n",
       "        <td>de_c4w4a2_silver_db</td>\n",
       "        <td>{&quot;IAM_ROLE&quot;:&quot;arn:aws:iam::533267227814:role/de-c4w4a2-load-role&quot;}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>dev</td>\n",
       "        <td>information_schema</td>\n",
       "        <td>1</td>\n",
       "        <td>local</td>\n",
       "        <td>rdsdb=UCDA/rdsdb~=U/rdsdb</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>dev</td>\n",
       "        <td>public</td>\n",
       "        <td>1</td>\n",
       "        <td>local</td>\n",
       "        <td>rdsdb=UCDA/rdsdb~=UC/rdsdb</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('dev', 'deftunes_serving', 100, 'local', None, None, None),\n",
       " ('dev', 'deftunes_transform', 100, 'external', None, 'de_c4w4a2_silver_db', '{\"IAM_ROLE\":\"arn:aws:iam::533267227814:role/de-c4w4a2-load-role\"}'),\n",
       " ('dev', 'information_schema', 1, 'local', 'rdsdb=UCDA/rdsdb~=U/rdsdb', None, None),\n",
       " ('dev', 'public', 1, 'local', 'rdsdb=UCDA/rdsdb~=UC/rdsdb', None, None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SHOW SCHEMAS FROM DATABASE dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was set up correctly, there should be three tables in the `deftunes_transform` schema in Redshift. Run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql+psycopg2://defaultuser:***@de-c4w4a2-redshift-cluster.ccr4tux8asfe.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>database_name</th>\n",
       "        <th>schema_name</th>\n",
       "        <th>table_name</th>\n",
       "        <th>table_type</th>\n",
       "        <th>table_acl</th>\n",
       "        <th>remarks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>dev</td>\n",
       "        <td>deftunes_transform</td>\n",
       "        <td>sessions</td>\n",
       "        <td>EXTERNAL TABLE</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>dev</td>\n",
       "        <td>deftunes_transform</td>\n",
       "        <td>songs</td>\n",
       "        <td>EXTERNAL TABLE</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>dev</td>\n",
       "        <td>deftunes_transform</td>\n",
       "        <td>users</td>\n",
       "        <td>EXTERNAL TABLE</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('dev', 'deftunes_transform', 'sessions', 'EXTERNAL TABLE', None, None),\n",
       " ('dev', 'deftunes_transform', 'songs', 'EXTERNAL TABLE', None, None),\n",
       " ('dev', 'deftunes_transform', 'users', 'EXTERNAL TABLE', None, None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SHOW TABLES FROM SCHEMA dev.deftunes_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Data Quality with AWS Glue\n",
    "\n",
    "To perform the data quality checks, we will use AWS Glue Data Quality which provides a good integration with tables in the Glue Data Catalog, AWS Glue ETL Jobs and Airflow. We will use the Data Quality service for the Data Catalog on top of the `transform` layer. This requires that the tables already exist, a rule set is defined for the quality checks to be performed on the table, and an IAM role is in place to run the rule set evaluation.\n",
    "\n",
    "Most of the configuration has been already set up on Terraform under the `data_quality` module, and we will define the rule set for each table and apply the corresponding module. To define the quality checks, we will use `Data Quality Definition Language (DQDL)`, these are some examples:\n",
    "\n",
    "- **ColumnDataType**: Checks if a column is compliant with a datatype.\n",
    "  - *Syntax*: `ColumnDataType <COL_NAME> = <EXPECTED_TYPE>`\n",
    "  - *Example*: `ColumnDataType \"colA\" = \"INTEGER\"`\n",
    "- **ColumnExists**: Checks if columns exist in a dataset.\n",
    "  - *Syntax*: `ColumnExists <COL_NAME>`\n",
    "  - *Example*: `ColumnExists \"Middle_Name\"`\n",
    "- **ColumnLength**: Checks if the length of data is consistent.\n",
    "  - *Syntax*: `ColumnLength <COL_NAME><EXPRESSION>`\n",
    "  - *Example:* `ColumnLength \"Postal_Code\" = 5`\n",
    "- **ColumnValues**: Runs an expression against the values in a column.\n",
    "  - *Syntax*: `ColumnValues <COL_NAME> <EXPRESSION>`\n",
    "  - *Example*: `ColumnValues \"Country\" in [ \"US\", \"CA\", \"UK\", NULL, EMPTY, WHITESPACES_ONLY ]`\n",
    "- **Completeness**: Checks the percentage of complete (non-null) values in a column.\n",
    "  - *Syntax*: `Completeness <COL_NAME> <EXPRESSION>`\n",
    "  - *Example*: `Completeness \"First_Name\" > 0.95`\n",
    "- **IsComplete**: Checks whether all of the values in a column are complete (non-null).\n",
    "  - *Syntax*: `IsComplete <COL_NAME>`\n",
    "  - *Example*: `IsComplete \"email\"` OR `IsComplete \"Email\" where \"Customer_ID between 1 and 50\"`\n",
    "- **IsPrimaryKey**: Checks whether a column contains a primary key.\n",
    "  - *Syntax*: `IsPrimaryKey <COL_NAME>`\n",
    "  - *Example*: `IsPrimaryKey \"Customer_ID\"`\n",
    "- **IsUnique**: Checks whether all of the values in a column are unique, and returns a boolean value.\n",
    "  - *Syntax*: `IsUnique <COL_NAME>`\n",
    "  - *Example*: `IsUnique \"email\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<a id='3-1'></a>\n",
    "### 3.1 - Configuring the Rule Sets\n",
    "\n",
    "This is an example of how to create a rule set for a table in the data catalog in Terraform, in the `ruleset` argument we pass a list of Rules separated by commas, in the `target_table` argument you point to the target table in the Glue Catalog:\n",
    "\n",
    "```bash\n",
    "resource \"aws_glue_data_quality_ruleset\" \"example\" {\n",
    "  name    = \"example\"\n",
    "  ruleset = \"Rules = [ IsComplete \\\"user_id\\\", IsComplete \\\"session_id\\\"]\"\n",
    "  target_table {\n",
    "    database_name = aws_glue_catalog_database.example.name\n",
    "    table_name    = aws_glue_catalog_table.example.name\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "3.1.1. Open the glue file `terraform/modules/data_quality/glue.tf`. \n",
    "\n",
    "In the resource \"aws_glue_data_quality_ruleset\" \"sessions_dq_ruleset\" we set the rule set (string) with\n",
    "\n",
    "```bash\n",
    "IsComplete \\\"user_id\\\", IsComplete \\\"session_id\\\", ColumnLength \\\"user_id\\\" = 36, ColumnLength \\\"session_id\\\" = 36, IsComplete \\\"song_id\\\", ColumnValues \\\"price\\\" <= 2\n",
    "```\n",
    "\n",
    "In the resource \"aws_glue_data_quality_ruleset\" \"users_dq_ruleset\" we set the rule set (string) with\n",
    "\n",
    "```bash\n",
    "IsComplete \\\"user_id\\\", Uniqueness \\\"user_id\\\" > 0.95, IsComplete \\\"user_lastname\\\", IsComplete \\\"user_name\\\", IsComplete \\\"user_since\\\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "3.1.2. Run the Terraform module `data_quality` with the following command:\n",
    "\n",
    "```bash\n",
    "terraform apply -target=module.data_quality\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<a id='3-2'></a>\n",
    "### 3.2 - Creating Materialized Views with *dbt*\n",
    "\n",
    "Before we configure and create the orchestration and dashboard, we have to create some materialized views to address some business questions, for this purpose we will use **dbt** to perform the required data modeling. For now, the only requirements in terms of views are related to aggregations of total sales per artists and per year and month. We will create these views on top of the current star schema in the serving layer; luckily `dbt` has already been set up to run alongside the Redshift cluster for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.1. In the `./dbt_modelling/models` folder, we will create a new subfolder called `bi_views`. Inside the new subfolder, create a `schema.yml` file to define the schema for the views:\n",
    "\n",
    "```yaml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: sales_per_country_vw\n",
    "    description: \"Sales per country view\"\n",
    "    columns:\n",
    "      - name: session_month\n",
    "      - name: session_year\n",
    "      - name: country_code\n",
    "      - name: total_sales\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2. Create file `sales_per_artist_vw.sql` in the same folder and we use the following `SELECT` statement for the model:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "date_part('year', fs.session_start_time) AS session_year,\n",
    "da.artist_name,\n",
    "SUM(fs.price) AS total_sales\n",
    "FROM {{var(\"target_schema\")}}.fact_session fs\n",
    "LEFT JOIN {{var(\"target_schema\")}}.dim_artists da\n",
    "ON fs.artist_id = da.artist_id\n",
    "GROUP BY 1,2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.3. Create file `sales_per_country_vw.sql` in the same folder and we use the following `SELECT` statement for the model:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "date_part('month', fs.session_start_time) AS session_month,\n",
    "date_part('year', fs.session_start_time) AS session_year,\n",
    "du.country_code,\n",
    "SUM(fs.price) AS total_sales\n",
    "FROM {{var(\"target_schema\")}}.fact_session fs\n",
    "LEFT JOIN {{var(\"target_schema\")}}.dim_users du\n",
    "ON fs.user_id = du.user_id\n",
    "GROUP BY 1,2,3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "3.2.4. In the file  `./dbt_modelling/dbt_project.yml`, we have the following variables in the file that simplify definition of the model for each view:\n",
    "\n",
    "```yaml\n",
    "vars:\n",
    "  source_schema: deftunes_transform\n",
    "  target_schema: deftunes_serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "3.2.5. Add the `bi_views` model in the models section.\n",
    "\n",
    "```yaml\n",
    "    bi_views:\n",
    "      +materialized: view\n",
    "      +schema: bi_views\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.6. Go to CloudFormation Outputs tab and we will have the value of the key `DBTBucket`. We replace the placeholders `<DBTBucket>` with it below. Run the commands in the terminal to copy the new models and `dbt_project.yml` into the S3 bucket. The Airflow pipelines will point to this bucket when they run *dbt* as a final step of their processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "aws s3 cp $HOME/project/dbt_modeling/models/bi_views s3://<DBTBucket>/dbt_project/dbt_modeling/models/bi_views --recursive\n",
    "aws s3 cp $HOME/project/dbt_modeling/dbt_project.yml s3://<DBTBucket>/dbt_project/dbt_modeling/dbt_project.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Orchestration with Apache Airflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-1'></a>\n",
    "### 4.1 - Accessing Apache Airflow\n",
    "\n",
    "4.1.1. In CloudFormation Outputs tab, search for the `AirflowDNS`; we then copy the value and paste it into another browser tab. This is the Apache Airflow environment that we can use to develop and run our dags. In the login page, use `airflow` for both the user and password. We will see that there is already one DAG named `deftunes_songs_pipeline_dag`; this is an example DAG to show the usage of several types of operators: [the `GlueJobOperator`](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_api/airflow/providers/amazon/aws/operators/glue/index.html#airflow.providers.amazon.aws.operators.glue.GlueJobOperator), [the `GlueDataQualityRuleSetEvaluationRunOperator`](https://airflow.apache.org/docs/apache-airflow-providers-amazon/8.26.0/_api/airflow/providers/amazon/aws/operators/glue/index.html#airflow.providers.amazon.aws.operators.glue.GlueDataQualityRuleSetEvaluationRunOperator), and [`DockerOperator`](https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/_api/airflow/providers/docker/operators/docker/index.html#airflow.providers.docker.operators.docker.DockerOperator).\n",
    "\n",
    "*Note:* In case Apache Airflow environment presents any issues, follow these instructions to restart the service:\n",
    "\n",
    "* In the `scripts` folder, we will see an script named `restart_airflow.sh`. Open it and copy its content.\n",
    "* Search for `CloudShell` in the AWS Console.\n",
    "* Create a new file named `restart_airflow.sh`. You can do it with `nano -c restart_airflow.sh`.\n",
    "* Copy the content in your clipboard into that file. Inside the `nano` editor you can save and exit with Ctrl + O and Ctrl + X or Cmd + O and Cmd + X, depending on the OS.\n",
    "* Run the script with:\n",
    "    ```bash\n",
    "    bash ./restart_airflow.sh\n",
    "    ```\n",
    "* This process will end when the service is healthy. That should take less than 3 minutes. Wait until the service has been restarted. Then, go to the Airflow UI and refresh the browser's tab until we can see the UI Again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-2'></a>\n",
    "### 4.2 - DAG for Songs Data in RDS Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1. Open the file `./dags/deftunes_songs_pipeline.py`. This file corresponds to the `deftunes_songs_pipeline_dag` that is already deployed in Airflow and has the tasks dependencies shown in the image:\n",
    "\n",
    "![songs_dag](./images/deftunes_songs_dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2. The deployed DAG is not completed. We replace the following placeholders with the values of the Terraform outputs:\n",
    "\n",
    "- `<DATA-LAKE-BUCKET>` with the value of `data_lake_bucket`.\n",
    "- `<SCRIPTS-BUCKET>` with the value of `scripts_bucket`.\n",
    "- `<GLUE-EXECUTION-ROLE>` with the value of `glue_role_arn`, full ARN.\n",
    "\n",
    "We can read through the file to understand the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3. Let's update and execute this DAG. Exchange the placeholder `<DAGS-BUCKET>` with the value of the key `DagsBucket` in CloudFormation Outputs and run the command in the terminal:\n",
    "\n",
    "```bash\n",
    "aws s3 cp $HOME/project/dags/deftunes_songs_pipeline.py s3://<DAGS-BUCKET>/dags/deftunes_songs_pipeline.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.4. Then, go to the Airflow UI. We press the toggle button to unpause it and it should start automatically. This DAG should perform a backfilling process for 2 months. After we have one successful run, click on each task and in the logs of each task we may understand what they are doing. We will see similar logs to the following ones:\n",
    "\n",
    "- **Glue jobs**. Example: Logs from the Glue jobs can look similar to the following: \n",
    "\n",
    "![glue_job_task](./images/glue_job_task_output.png)\n",
    "\n",
    "We can see that the `GlueJobOperator` is continuously requesting the status of the current glue job run until it is `SUCCEEDED` or `FAILED`.\n",
    "\n",
    "- **Data Quality checks**. Example: Logs from Data quality check tasks can look similar to: \n",
    "\n",
    "![data_quality_task](./images/data_quality_task_output.png)\n",
    "\n",
    "The `GlueDataQualityRuleSetEvaluationRunOperator` will show some metrics regarding the quality rules that we created and will show a `PASS` or `FAILED` message for each column depending on if a particular column holds the imposed rule or not.\n",
    "\n",
    "- **DBT with DockerOperator**. The Logs from the `DockerOperator` that runs `dbt` will look like the following image:\n",
    "\n",
    "![dbt_task](./images/dbt_task_output.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-3'></a>\n",
    "### 4.3 - DAG for Users and Sessions Data from API Source\n",
    "\n",
    "Now we need to create the DAG to orchestrate the extraction, transformation, quality checks and creation of star schema for the data obtained from the two API endpoints: users and sessions. This is the diagram:\n",
    "\n",
    "![deftunes_api_dag](./images/deftunes_api_dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.1. Open the file `./dags/deftunes_api_pipeline.py`. We replace the following placeholders with the values of the Terraform outputs:\n",
    "\n",
    "- `<DATA-LAKE-BUCKET>` with the value of `data_lake_bucket`.\n",
    "- `<SCRIPTS-BUCKET>` with the value of `scripts_bucket`.\n",
    "- `<GLUE-EXECUTION-ROLE>` with the value of `glue_role_arn`, full ARN.\n",
    "\n",
    "Also, we replace `<API-ENDPOINT>` placeholder with the value of the key `APIEndpoint` in CloudFormation Outputs.\n",
    "\n",
    "We go through the file with the comments to understand the tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.2. We exchange the placeholder `<DAGS-BUCKET>` with the value of the key `DagsBucket` in CloudFormation Outputs and run the command in the terminal:\n",
    "\n",
    "```bash\n",
    "aws s3 cp $HOME/project/dags/deftunes_api_pipeline.py s3://<DAGS-BUCKET>/dags/deftunes_api_pipeline.py\n",
    "```\n",
    "\n",
    "Now, go to our Airflow UI and refresh it or wait until we see the new DAG named `deftunes_api_pipeline_dag`. Click on the toggle button to unpause it and it should start executing automatically. It will perform a backfilling, extracting the data from the first available date. This DAG should perform a backfilling process for 3 months.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5 - Data Visualization with Apache Superset (Or another compatible dashboarding tool)\n",
    "\n",
    "Finally, to incorporate data visualization to the current data architecture, an EC2 instance has been set up to work with Apache Superset, and the URL is among the CloudFormation outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. Access the Superset UI using the URL provided in the CloudFormation Outputs.\n",
    "Login using the following credentials: \n",
    "\n",
    "* `user`: admin\n",
    "* `password`: admin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. Now, we will configure the Redshift connection: click on the dropdown `Settings` menu in the top right, under the Data section select `Data Connections`. Click on the top right `+ Database` button, a new menu should appear to configure the new connection.\n",
    "Use the `Choose database...` dropdown menu, the first option will be **Amazon Redshift**; select it and click Next. Go down to the link that says `Connect this database with a SQLAlchemy URI string instead`; click it and then input the string inputted by the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(redshift_connection_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3. We can test the connection or just click on the CONNECT button to finally perform the connection. After that, select the Datasets tab in the top header menu and we will be directed to the following page.\n",
    "\n",
    "Click on the `+ DATASET` button on the top right, and a new screen will appear; we can use the connection that you just configured for Amazon Redshift, then select the business views schema `deftunes_bi_views` and finally one of the views.\n",
    "\n",
    "Then click on the **CREATE DATASET AND CREATE CHART** button; we will be directed to a new page to create a chart based on the dataset. Once we are done with the chart, hit the **Save** button on the top right, and it will ask us to give the chart a name and then save it. Create a chart for each view, then create a new dashboard in the *Dashboards* section of the top navigational header, using the `+ Dashboard` button. Enter a name (in the top left part) for our dashboard and then drag and drop the charts we created earlier onto the dashboard canvas. Resize and arrange the charts as desired to create our dashboard layout and finally click **Save** to save the dashboard layout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we are done with our project!**\n",
    "\n",
    "During the second part of the project, we have enhanced the existing data architecture for Music Depot's new business operation. We implemented data quality checks to systematically evaluate the cleansed data, and added orchestration among the various pipeline components. We have successfully experimented with a set of data engineering tools to develop a comprehensive data project. In the future, we will be required to work with similar tools. It is essential to understand the underlying principles and requirements of each tool, and, in due time, incorporate them into our skill set as a data engineer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
